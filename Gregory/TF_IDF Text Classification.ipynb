{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import sklearn\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_data = pd.read_csv('/Users/gregdecanio/Desktop/CAP4770/Group7_Project/CAP4770Group7/new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#plot = comment_data['target'].hist(bins=20)\n",
    "#print comment_data['target'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Discrete Classification to Data\n",
    "### I will be classifying the data into 4 categories:\n",
    "Not toxic: target < 0.5  \n",
    "Toxic: target >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def classifier(row):\n",
    "  if row['target'] >= 0.5:\n",
    "    return 'toxic'\n",
    "  elif row['target'] < 0.5:\n",
    "    return 'not-toxic'\n",
    "  else:\n",
    "    return 'undefined'\n",
    "\n",
    "comment_data['toxicity_classification'] = comment_data.apply(classifier, axis=1)\n",
    "comment_data.groupby(['toxicity_classification']).size().plot(kind='bar')\n",
    "print comment_data.groupby(['toxicity_classification']).size()\n",
    "print \"Non-toxic count: \", comment_data.groupby(['toxicity_classification']).size()['not-toxic']\n",
    "print \"Toxic count: \", comment_data.groupby(['toxicity_classification']).size()['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Comment Text\n",
    "## Removing Stop Words\n",
    "https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python  \n",
    "https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "## Removing Punctuation\n",
    "https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string  \n",
    "According to the Stack Overflow, I could probably configure a more efficient way to remove punctuation. However, what I have currently works fine and is adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Removing stop words from the comments, where stop words are defined in NLTK stop words dictionary\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words(\"english\")\n",
    "\n",
    "def removeStopWordsInComment(row):\n",
    "    return ' '.join([word for word in row['comment_text'].split() if word not in stopWords])\n",
    "    \n",
    "comment_data['comment_no_stop_words'] = ''\n",
    "comment_data['comment_no_stop_words'] = comment_data.apply(removeStopWordsInComment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Removing puncuation from the comments, where punctuation is defined by the STRING punctuation dictionary\n",
    "import string\n",
    "punc = set(string.punctuation)\n",
    "\n",
    "def removePunctuation(row):\n",
    "    return ''.join([ch for ch in row['comment_no_stop_words'] if ch not in punc])\n",
    "\n",
    "comment_data['comment_no_stop_punc'] = ''\n",
    "comment_data['comment_no_stop_punc'] = comment_data.apply(removePunctuation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_data[['comment_text', 'target', 'toxicity_classification','comment_no_stop_words', 'comment_no_stop_punc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data Into Test/Train Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(comment_data, test_size=0.33, random_state=42)\n",
    "print \"Training data size: \", len(train)\n",
    "print \"Testing data size: \", len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[['comment_text', 'target', 'toxicity_classification', 'comment_no_stop_punc']]\n",
    "test = test[['comment_text', 'target', 'toxicity_classification', 'comment_no_stop_punc']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_unclean_vect = CountVectorizer()\n",
    "tfidf_unclean_transformer = TfidfTransformer()\n",
    "\n",
    "count_clean_vect = CountVectorizer()\n",
    "tfidf_clean_transformer = TfidfTransformer()\n",
    "\n",
    "#Performing TF and TF-IDF transformation on training data\n",
    "\n",
    "train_unclean_counts = count_unclean_vect.fit_transform(train['comment_text'])\n",
    "print \"Dimensions of Unclean Training Data Document Term Matrix: \", train_unclean_counts.shape\n",
    "train_unclean_tfidf = tfidf_unclean_transformer.fit_transform(train_unclean_counts)\n",
    "print \"Dimensions of Unclean Training Data TF_IDF Matrix: \", train_unclean_tfidf.shape\n",
    "\n",
    "train_clean_counts = count_clean_vect.fit_transform(train['comment_no_stop_punc'])\n",
    "print \"Dimensions of Clean Training Data Document Term Matrix: \", train_clean_counts.shape\n",
    "train_clean_tfidf = tfidf_clean_transformer.fit_transform(train_clean_counts)\n",
    "print \"Dimensions of Clean Training Data TF_IDF Matrix: \", train_clean_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing TF and TF-IDF transformation on test data\n",
    "\n",
    "test_unclean_counts = count_unclean_vect.transform(test['comment_text'])\n",
    "print \"Dimensions of Unclean Test Data Document Term Matrix: \", test_unclean_counts.shape\n",
    "test_unclean_tfidf = tfidf_unclean_transformer.transform(test_unclean_counts)\n",
    "print \"Dimensions of Unclean Test Data TF_IDF Matrix: \", test_unclean_tfidf.shape\n",
    "\n",
    "test_clean_counts = count_clean_vect.transform(test['comment_no_stop_punc'])\n",
    "print \"Dimensions of Clean Test Data Document Term Matrix: \", test_clean_counts.shape\n",
    "test_clean_tfidf = tfidf_clean_transformer.transform(test_clean_counts)\n",
    "print \"Dimensions of Clean Test Data TF_IDF Matrix: \", test_clean_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "https://scikit-learn.org/0.19/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Training and creating the Multinomial Naive Bayes model (unclean)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "modelNB_unclean = MultinomialNB().fit(train_unclean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Training and creating the Multinomial Naive Bayes model (clean)\n",
    "modelNB_clean = MultinomialNB().fit(train_clean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Calulate accuracy of predictions\n",
    "predsNB_unclean = modelNB_unclean.predict(test_unclean_tfidf)\n",
    "accNB_unclean = np.mean(predsNB_unclean == test['toxicity_classification']) \n",
    "print \"Accuracy of Naive Bayes Classifier (Unclean): \", accNB_unclean, \"\\n\"\n",
    "\n",
    "predsNB_clean = modelNB_clean.predict(test_clean_tfidf)\n",
    "accNB_clean = np.mean(predsNB_clean == test['toxicity_classification']) \n",
    "print \"Accuracy of Naive Bayes Classifier (Clean): \", accNB_clean, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Show confustion matrix of predictions\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsNB_unclean))\n",
    "print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsNB_unclean)\n",
    "\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsNB_clean))\n",
    "print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsNB_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classification\n",
    "Currently # of neighbors is arbitraily set to 2 (Greg, 11/20/2019)  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier  \n",
    "https://scikit-learn.org/stable/modules/neighbors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Training and creating the K Nearest Neighbors model (unclean)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "modelKNN_unclean = KNeighborsClassifier(n_neighbors=2, algorithm='brute').fit(train_unclean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Training and creating the K Nearest Neighbors model (clean)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "modelKNN_clean = KNeighborsClassifier(n_neighbors=2, algorithm='brute').fit(train_clean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Calulate accuracy of predictions\n",
    "#predsKNN_unclean = modelKNN_unclean.predict(test_unclean_tfidf)\n",
    "#accKNN_unclean = np.mean(predsKNN_unclean == test['toxicity_classification'])\n",
    "#print \"Accuracy of K Nearest Neighbors Classifier (Unclean): \", accKNN_unclean\n",
    "\n",
    "#predsKNN_clean = modelKNN_unclean.predict(test_clean_tfidf)\n",
    "#accKNN_clean = np.mean(predsKNN_clean == test['toxicity_classification'])\n",
    "#print \"Accuracy of K Nearest Neighbors Classifier (Unclean): \", accKNN_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Show confustion matrix of predictions\n",
    "#print(metrics.classification_report(test['toxicity_classification'], predsKNN_unclean))\n",
    "#print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsKNN_unclean)\n",
    "\n",
    "#print(metrics.classification_report(test['toxicity_classification'], predsKNN_clean))\n",
    "#print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsKNN_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC Classification\n",
    "https://scikit-learn.org/stable/modules/svm.html#classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Training and creating the SVC model (unclean)\n",
    "#from sklearn import svm\n",
    "#modelSVC_unclean = svm.SVC(kernel='linear').fit(train_unclean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Training and creating the SVC model (clean)\n",
    "#modelSVC_clean = svm.SVC(kernel='linear').fit(train_clean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Calulate accuracy of predictions\n",
    "#predsSVC_unclean = modelSVC_unclean.predict(test_unclean_tfidf)\n",
    "#accSVC_unclean = np.mean(predsSVC_unclean == test['toxicity_classification'])\n",
    "#print \"Accuracy of SCV Classifier (Unclean): \", accSVC_unclean\n",
    "\n",
    "#predsSVC_clean = modelSVC_clean.predict(test_clean_tfidf)\n",
    "#accSVC_clean = np.mean(predsSVC_clean == test['toxicity_classification'])\n",
    "#print \"Accuracy of SVC Classifier (Clean): \", accSVC_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Show confustion matrix of predictions\n",
    "#print(metrics.classification_report(test['toxicity_classification'], predsSVC_unclean))\n",
    "#print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsSVC_unclean)\n",
    "\n",
    "#print(metrics.classification_report(test['toxicity_classification'], predsSVC_clean))\n",
    "#print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsSVC_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# SGD Classification\n",
    "https://scikit-learn.org/stable/modules/sgd.html#classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Training and creating the SGD Classifier model (unclean)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "modelSGD_unclean = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           max_iter=5).fit(train_unclean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Training and creating the SGD Classifier model (clean)\n",
    "modelSGD_clean = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           max_iter=5).fit(train_clean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Calulate accuracy of predictions\n",
    "predsSGD_unclean = modelSGD_unclean.predict(test_unclean_tfidf)\n",
    "accSGD_unclean = np.mean(predsSGD_unclean == test['toxicity_classification']) \n",
    "print \"Accuracy of SGD Classifier (Unclean): \", accSGD_unclean, \"\\n\"\n",
    "\n",
    "predsSGD_clean = modelSGD_clean.predict(test_clean_tfidf)\n",
    "accSGD_clean = np.mean(predsSGD_clean == test['toxicity_classification']) \n",
    "print \"Accuracy of SGD Classifier (Clean): \", accSGD_clean, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Show confustion matrix of predictions\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsSGD_unclean))\n",
    "print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsSGD_unclean)\n",
    "\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsSGD_clean))\n",
    "print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsSGD_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How a Pipeline Is Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('model', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
