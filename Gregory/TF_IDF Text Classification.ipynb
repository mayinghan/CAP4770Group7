{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import sklearn\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_data = pd.read_csv('/Users/gregdecanio/Desktop/CAP4770/Group7_Project/CAP4770Group7/new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#plot = comment_data['target'].hist(bins=20)\n",
    "#print comment_data['target'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Discrete Classification to Data\n",
    "### I will be classifying the data into 4 categories:\n",
    "Not toxic: target < 0.5  \n",
    "Toxic: target >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity_classification\n",
      "not-toxic    96851\n",
      "toxic        18149\n",
      "dtype: int64\n",
      "Non-toxic count:  96851\n",
      "Toxic count:  18149\n",
      "CPU times: user 4.68 s, sys: 231 ms, total: 4.91 s\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def classifier(row):\n",
    "  if row['target'] >= 0.5:\n",
    "    return 'toxic'\n",
    "  elif row['target'] < 0.5:\n",
    "    return 'not-toxic'\n",
    "  else:\n",
    "    return 'undefined'\n",
    "\n",
    "comment_data['toxicity_classification'] = comment_data.apply(classifier, axis=1)\n",
    "comment_data.groupby(['toxicity_classification']).size().plot(kind='bar')\n",
    "print comment_data.groupby(['toxicity_classification']).size()\n",
    "print \"Non-toxic count: \", comment_data.groupby(['toxicity_classification']).size()['not-toxic']\n",
    "print \"Toxic count: \", comment_data.groupby(['toxicity_classification']).size()['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Comment Text\n",
    "## Removing Stop Words\n",
    "https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python  \n",
    "https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregdecanio/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 883 ms, total: 1min 14s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Removing stop words from the comments, where stop words are defined in NLTK stop words dictionary\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words(\"english\")\n",
    "\n",
    "def removeStopWordsInComment(row):\n",
    "    return ' '.join([word for word in row['comment_text'].split() if word not in stopWords])\n",
    "    \n",
    "comment_data['comment_no_stop_words'] = ''\n",
    "comment_data['comment_no_stop_words'] = comment_data.apply(removeStopWordsInComment, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation\n",
    "https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string  \n",
    "According to the Stack Overflow, I could probably configure a more efficient way to remove punctuation. However, what I have currently works fine and is adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 307 ms, total: 10.7 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Removing puncuation from the comments, where punctuation is defined by the STRING punctuation dictionary\n",
    "import string\n",
    "punc = set(string.punctuation)\n",
    "\n",
    "def removePunctuation(row):\n",
    "    return ''.join([ch for ch in row['comment_no_stop_words'] if ch not in punc])\n",
    "\n",
    "comment_data['comment_no_stop_punc'] = ''\n",
    "comment_data['comment_no_stop_punc'] = comment_data.apply(removePunctuation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_data[['comment_text', 'target', 'toxicity_classification','comment_no_stop_words', 'comment_no_stop_punc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data Into Test/Train Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  77050\n",
      "Testing data size:  37950\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(comment_data, test_size=0.33, random_state=42)\n",
    "print \"Training data size: \", len(train)\n",
    "print \"Testing data size: \", len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "      <th>toxicity_classification</th>\n",
       "      <th>comment_no_stop_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25219</th>\n",
       "      <td>Your quote from the article really describes A...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>not-toxic</td>\n",
       "      <td>Your quote article really describes Arab Sprin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97436</th>\n",
       "      <td>The Crooks always seems to con there way in an...</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>toxic</td>\n",
       "      <td>The Crooks always seems con way win debate kan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>Many illegal aliens are white hispanics.Many w...</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>toxic</td>\n",
       "      <td>Many illegal aliens white hispanicsMany white ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16948</th>\n",
       "      <td>Total misrepresentation. Why was Bernie agains...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>not-toxic</td>\n",
       "      <td>Total misrepresentation Why Bernie it What AFL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89159</th>\n",
       "      <td>When was this announced?</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>not-toxic</td>\n",
       "      <td>When announced</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comment_text  target  \\\n",
       "25219  Your quote from the article really describes A...  0.0000   \n",
       "97436  The Crooks always seems to con there way in an...  0.6500   \n",
       "4346   Many illegal aliens are white hispanics.Many w...  0.5375   \n",
       "16948  Total misrepresentation. Why was Bernie agains...  0.0000   \n",
       "89159                           When was this announced?  0.0000   \n",
       "\n",
       "      toxicity_classification  \\\n",
       "25219               not-toxic   \n",
       "97436                   toxic   \n",
       "4346                    toxic   \n",
       "16948               not-toxic   \n",
       "89159               not-toxic   \n",
       "\n",
       "                                    comment_no_stop_punc  \n",
       "25219  Your quote article really describes Arab Sprin...  \n",
       "97436  The Crooks always seems con way win debate kan...  \n",
       "4346   Many illegal aliens white hispanicsMany white ...  \n",
       "16948  Total misrepresentation Why Bernie it What AFL...  \n",
       "89159                                     When announced  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['comment_text', 'target', 'toxicity_classification', 'comment_no_stop_punc']]\n",
    "test = test[['comment_text', 'target', 'toxicity_classification', 'comment_no_stop_punc']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Unclean Training Data Document Term Matrix:  (77050, 69165)\n",
      "Dimensions of Unclean Training Data TF_IDF Matrix:  (77050, 69165)\n",
      "Dimensions of Clean Training Data Document Term Matrix:  (77050, 90324)\n",
      "Dimensions of Clean Training Data TF_IDF Matrix:  (77050, 90324)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_unclean_vect = CountVectorizer()\n",
    "tfidf_unclean_transformer = TfidfTransformer()\n",
    "\n",
    "count_clean_vect = CountVectorizer()\n",
    "tfidf_clean_transformer = TfidfTransformer()\n",
    "\n",
    "#Performing TF and TF-IDF transformation on training data\n",
    "\n",
    "train_unclean_counts = count_unclean_vect.fit_transform(train['comment_text'])\n",
    "print \"Dimensions of Unclean Training Data Document Term Matrix: \", train_unclean_counts.shape\n",
    "train_unclean_tfidf = tfidf_unclean_transformer.fit_transform(train_unclean_counts)\n",
    "print \"Dimensions of Unclean Training Data TF_IDF Matrix: \", train_unclean_tfidf.shape\n",
    "\n",
    "train_clean_counts = count_clean_vect.fit_transform(train['comment_no_stop_punc'])\n",
    "print \"Dimensions of Clean Training Data Document Term Matrix: \", train_clean_counts.shape\n",
    "train_clean_tfidf = tfidf_clean_transformer.fit_transform(train_clean_counts)\n",
    "print \"Dimensions of Clean Training Data TF_IDF Matrix: \", train_clean_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Unclean Test Data Document Term Matrix:  (37950, 69165)\n",
      "Dimensions of Unclean Test Data TF_IDF Matrix:  (37950, 69165)\n",
      "Dimensions of Clean Test Data Document Term Matrix:  (37950, 90324)\n",
      "Dimensions of Clean Test Data TF_IDF Matrix:  (37950, 90324)\n"
     ]
    }
   ],
   "source": [
    "#Performing TF and TF-IDF transformation on test data\n",
    "\n",
    "test_unclean_counts = count_unclean_vect.transform(test['comment_text'])\n",
    "print \"Dimensions of Unclean Test Data Document Term Matrix: \", test_unclean_counts.shape\n",
    "test_unclean_tfidf = tfidf_unclean_transformer.transform(test_unclean_counts)\n",
    "print \"Dimensions of Unclean Test Data TF_IDF Matrix: \", test_unclean_tfidf.shape\n",
    "\n",
    "test_clean_counts = count_clean_vect.transform(test['comment_no_stop_punc'])\n",
    "print \"Dimensions of Clean Test Data Document Term Matrix: \", test_clean_counts.shape\n",
    "test_clean_tfidf = tfidf_clean_transformer.transform(test_clean_counts)\n",
    "print \"Dimensions of Clean Test Data TF_IDF Matrix: \", test_clean_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "https://scikit-learn.org/0.19/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 251 ms, sys: 49.2 ms, total: 300 ms\n",
      "Wall time: 315 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training and creating the Multinomial Naive Bayes model (unclean)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "modelNB_unclean = MultinomialNB().fit(train_unclean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 252 ms, sys: 24.7 ms, total: 277 ms\n",
      "Wall time: 280 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training and creating the Multinomial Naive Bayes model (clean)\n",
    "modelNB_clean = MultinomialNB().fit(train_clean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes Classifier (Unclean):  0.845876152833 \n",
      "\n",
      "Accuracy of Naive Bayes Classifier (Clean):  0.84534914361 \n",
      "\n",
      "CPU times: user 45.8 ms, sys: 17.8 ms, total: 63.6 ms\n",
      "Wall time: 67 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Calulate accuracy of predictions\n",
    "predsNB_unclean = modelNB_unclean.predict(test_unclean_tfidf)\n",
    "accNB_unclean = np.mean(predsNB_unclean == test['toxicity_classification']) \n",
    "print \"Accuracy of Naive Bayes Classifier (Unclean): \", accNB_unclean, \"\\n\"\n",
    "\n",
    "predsNB_clean = modelNB_clean.predict(test_clean_tfidf)\n",
    "accNB_clean = np.mean(predsNB_clean == test['toxicity_classification']) \n",
    "print \"Accuracy of Naive Bayes Classifier (Clean): \", accNB_clean, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not-toxic       0.85      1.00      0.92     31988\n",
      "      toxic       0.91      0.02      0.04      5962\n",
      "\n",
      "avg / total       0.86      0.85      0.78     37950\n",
      "\n",
      "Confusion Matrix (Unclean):\n",
      "[[31975    13]\n",
      " [ 5836   126]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not-toxic       0.85      1.00      0.92     31988\n",
      "      toxic       0.95      0.02      0.03      5962\n",
      "\n",
      "avg / total       0.86      0.85      0.78     37950\n",
      "\n",
      "Confusion Matrix (Clean):\n",
      "[[31983     5]\n",
      " [ 5864    98]]\n",
      "CPU times: user 734 ms, sys: 12.9 ms, total: 747 ms\n",
      "Wall time: 743 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Show confustion matrix of predictions\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsNB_unclean))\n",
    "print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsNB_unclean)\n",
    "\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsNB_clean))\n",
    "print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsNB_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classification\n",
    "Currently # of neighbors is arbitraily set to 2 (Greg, 11/20/2019)  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier  \n",
    "https://scikit-learn.org/stable/modules/neighbors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "#Training and creating the K Nearest Neighbors model (unclean)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "modelKNN_unclean = KNeighborsClassifier(n_neighbors=2, algorithm='brute').fit(train_unclean_tfidf, train['toxicity_classification'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "#Training and creating the K Nearest Neighbors model (clean)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "modelKNN_clean = KNeighborsClassifier(n_neighbors=2, algorithm='brute').fit(train_clean_tfidf, train['toxicity_classification'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "#Calulate accuracy of predictions\n",
    "predsKNN_unclean = modelKNN_unclean.predict(test_unclean_tfidf)\n",
    "accKNN_unclean = np.mean(predsKNN_unclean == test['toxicity_classification'])\n",
    "print \"Accuracy of K Nearest Neighbors Classifier (Unclean): \", accKNN_unclean\n",
    "\n",
    "predsKNN_clean = modelKNN_unclean.predict(test_clean_tfidf)\n",
    "accKNN_clean = np.mean(predsKNN_clean == test['toxicity_classification'])\n",
    "print \"Accuracy of K Nearest Neighbors Classifier (Unclean): \", accKNN_clean\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "#Show confustion matrix of predictions\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsKNN_unclean))\n",
    "print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsKNN_unclean)\n",
    "\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsKNN_clean))\n",
    "print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsKNN_clean)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC Classification\n",
    "https://scikit-learn.org/stable/modules/svm.html#classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "#Training and creating the SVC model (unclean)\n",
    "from sklearn import svm\n",
    "modelSVC_unclean = svm.SVC(kernel='linear').fit(train_unclean_tfidf, train['toxicity_classification'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "#Training and creating the SVC model (clean)\n",
    "modelSVC_clean = svm.SVC(kernel='linear').fit(train_clean_tfidf, train['toxicity_classification'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "#Calulate accuracy of predictions\n",
    "predsSVC_unclean = modelSVC_unclean.predict(test_unclean_tfidf)\n",
    "accSVC_unclean = np.mean(predsSVC_unclean == test['toxicity_classification'])\n",
    "print \"Accuracy of SCV Classifier (Unclean): \", accSVC_unclean\n",
    "\n",
    "predsSVC_clean = modelSVC_clean.predict(test_clean_tfidf)\n",
    "accSVC_clean = np.mean(predsSVC_clean == test['toxicity_classification'])\n",
    "print \"Accuracy of SVC Classifier (Clean): \", accSVC_clean\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "#Show confustion matrix of predictions\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsSVC_unclean))\n",
    "print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsSVC_unclean)\n",
    "\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsSVC_clean))\n",
    "print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsSVC_clean)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# SGD Classification\n",
    "https://scikit-learn.org/stable/modules/sgd.html#classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 213 ms, sys: 9.4 ms, total: 222 ms\n",
      "Wall time: 241 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training and creating the SGD Classifier model (unclean)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "modelSGD_unclean = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           max_iter=5).fit(train_unclean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 216 ms, sys: 6.32 ms, total: 223 ms\n",
      "Wall time: 226 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training and creating the SGD Classifier model (clean)\n",
    "modelSGD_clean = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           max_iter=5).fit(train_clean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SGD Classifier (Unclean):  0.874993412385 \n",
      "\n",
      "Accuracy of SGD Classifier (Clean):  0.876574440053 \n",
      "\n",
      "CPU times: user 34 ms, sys: 3.14 ms, total: 37.1 ms\n",
      "Wall time: 39.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Calulate accuracy of predictions\n",
    "predsSGD_unclean = modelSGD_unclean.predict(test_unclean_tfidf)\n",
    "accSGD_unclean = np.mean(predsSGD_unclean == test['toxicity_classification']) \n",
    "print \"Accuracy of SGD Classifier (Unclean): \", accSGD_unclean, \"\\n\"\n",
    "\n",
    "predsSGD_clean = modelSGD_clean.predict(test_clean_tfidf)\n",
    "accSGD_clean = np.mean(predsSGD_clean == test['toxicity_classification']) \n",
    "print \"Accuracy of SGD Classifier (Clean): \", accSGD_clean, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not-toxic       0.87      1.00      0.93     31988\n",
      "      toxic       0.93      0.22      0.36      5962\n",
      "\n",
      "avg / total       0.88      0.87      0.84     37950\n",
      "\n",
      "Confusion Matrix (Unclean):\n",
      "[[31887   101]\n",
      " [ 4643  1319]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not-toxic       0.87      1.00      0.93     31988\n",
      "      toxic       0.92      0.23      0.37      5962\n",
      "\n",
      "avg / total       0.88      0.88      0.84     37950\n",
      "\n",
      "Confusion Matrix (Clean):\n",
      "[[31872   116]\n",
      " [ 4568  1394]]\n",
      "CPU times: user 731 ms, sys: 9.79 ms, total: 741 ms\n",
      "Wall time: 736 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Show confustion matrix of predictions\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsSGD_unclean))\n",
    "print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsSGD_unclean)\n",
    "\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsSGD_clean))\n",
    "print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsSGD_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification\n",
    "https://scikit-learn.org/0.19/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 s, sys: 273 ms, total: 33.3 s\n",
      "Wall time: 34.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training the data and creating the Random Forest model (unclean)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelRF_unclean = RandomForestClassifier().fit(train_unclean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.3 s, sys: 344 ms, total: 42.7 s\n",
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training the data and creating the Random Forest model (clean)\n",
    "modelRF_clean = RandomForestClassifier().fit(train_clean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RF Classifier (Unclean):  0.875467720685 \n",
      "\n",
      "Accuracy of RF Classifier (Clean):  0.885902503294 \n",
      "\n",
      "CPU times: user 3.87 s, sys: 29.7 ms, total: 3.9 s\n",
      "Wall time: 3.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Calulate accuracy of predictions\n",
    "predsRF_unclean = modelRF_unclean.predict(test_unclean_tfidf)\n",
    "accRF_unclean = np.mean(predsRF_unclean == test['toxicity_classification']) \n",
    "print \"Accuracy of RF Classifier (Unclean): \", accRF_unclean, \"\\n\"\n",
    "\n",
    "predsRF_clean = modelRF_clean.predict(test_clean_tfidf)\n",
    "accRF_clean = np.mean(predsRF_clean == test['toxicity_classification']) \n",
    "print \"Accuracy of RF Classifier (Clean): \", accRF_clean, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not-toxic       0.88      0.99      0.93     31988\n",
      "      toxic       0.84      0.26      0.39      5962\n",
      "\n",
      "avg / total       0.87      0.88      0.85     37950\n",
      "\n",
      "Confusion Matrix (Unclean):\n",
      "[[31688   300]\n",
      " [ 4426  1536]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not-toxic       0.89      0.98      0.94     31988\n",
      "      toxic       0.80      0.36      0.50      5962\n",
      "\n",
      "avg / total       0.88      0.89      0.87     37950\n",
      "\n",
      "Confusion Matrix (Clean):\n",
      "[[31446   542]\n",
      " [ 3788  2174]]\n",
      "CPU times: user 913 ms, sys: 11.8 ms, total: 925 ms\n",
      "Wall time: 923 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Show confustion matrix of predictions\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsRF_unclean))\n",
    "print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsRF_unclean)\n",
    "\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsRF_clean))\n",
    "print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsRF_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classification\n",
    "https://scikit-learn.org/0.19/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 17s, sys: 2.01 s, total: 6min 19s\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training the data and creating the Decision Tree model (unclean)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "modelDT_unclean = DecisionTreeClassifier().fit(train_unclean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 50s, sys: 619 ms, total: 5min 50s\n",
      "Wall time: 5min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training the data and creating the Decision Tree model (clean)\n",
    "modelDT_clean = DecisionTreeClassifier().fit(train_clean_tfidf, train['toxicity_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DT Classifier (Unclean):  0.87093544137 \n",
      "\n",
      "Accuracy of DT Classifier (Clean):  0.872516469038 \n",
      "\n",
      "CPU times: user 625 ms, sys: 59.4 ms, total: 684 ms\n",
      "Wall time: 690 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Calulate accuracy of predictions\n",
    "predsDT_unclean = modelDT_unclean.predict(test_unclean_tfidf)\n",
    "accDT_unclean = np.mean(predsDT_unclean == test['toxicity_classification']) \n",
    "print \"Accuracy of DT Classifier (Unclean): \", accDT_unclean, \"\\n\"\n",
    "\n",
    "predsDT_clean = modelDT_clean.predict(test_clean_tfidf)\n",
    "accDT_clean = np.mean(predsDT_clean == test['toxicity_classification']) \n",
    "print \"Accuracy of DT Classifier (Clean): \", accDT_clean, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not-toxic       0.92      0.93      0.92     31988\n",
      "      toxic       0.59      0.57      0.58      5962\n",
      "\n",
      "avg / total       0.87      0.87      0.87     37950\n",
      "\n",
      "Confusion Matrix (Unclean):\n",
      "[[29677  2311]\n",
      " [ 2587  3375]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not-toxic       0.92      0.93      0.92     31988\n",
      "      toxic       0.60      0.59      0.59      5962\n",
      "\n",
      "avg / total       0.87      0.87      0.87     37950\n",
      "\n",
      "Confusion Matrix (Clean):\n",
      "[[29620  2368]\n",
      " [ 2470  3492]]\n",
      "CPU times: user 908 ms, sys: 16.3 ms, total: 924 ms\n",
      "Wall time: 924 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Show confustion matrix of predictions\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsDT_unclean))\n",
    "print \"Confusion Matrix (Unclean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsDT_unclean)\n",
    "\n",
    "print(metrics.classification_report(test['toxicity_classification'], predsDT_clean))\n",
    "print \"Confusion Matrix (Clean):\\n\", metrics.confusion_matrix(test['toxicity_classification'], predsDT_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How a Pipeline Is Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('model', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    " ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
